{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA CREDIT HOMEWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal\n",
    "import seaborn as sns; sns.set()\n",
    "from demo import fairness_demo\n",
    "\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1  [25 points]\n",
    "Consider the four datasets returned from the function get_dataset($d$) for $d=2,3,4, \\& \\;5$. Here $d$ is the dimensionality of the non-sensitive covariates, which are returned in the matrix $X$, whereas the vectors $y$ and $x_{\\rm{sensitive}}$ store the target labels and sensitive covariate, respectively. (As such the structure of the data is exactly analagous to what we have in the fairness demo notebook from the lecture.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gaussian_data(mean, cov, class_label, n_samples):\n",
    "    nv = multivariate_normal(mean = mean, cov = cov)\n",
    "    X = nv.rvs(n_samples)\n",
    "    y = np.ones(n_samples, dtype=float) * class_label\n",
    "    return X,y\n",
    "\n",
    "### this function returns the dataset\n",
    "### e.g. X, y, x_sensitive = get_dataset(2)\n",
    "\n",
    "def get_dataset(d):\n",
    "    np.random.seed(5)\n",
    "    mu1 = 0.5*np.ones(d)\n",
    "    mu2 = -0.5*np.ones(d)\n",
    "    sigma1 = np.eye(d)\n",
    "    sigma2 = np.eye(d)\n",
    "    X1, y1 = get_gaussian_data(mu1, sigma1, 1, 10000*d) # positive class\n",
    "    X2, y2 = get_gaussian_data(mu2, sigma2, -1, 10000*d) # negative class\n",
    "\n",
    "    X = np.vstack((X1, X2)) # non-sensitive covariates\n",
    "    y = np.hstack((y1, y2)) # class labels\n",
    "    x_sensitive = np.ones(X.shape[0])\n",
    "    x_sensitive[X[:,0]<0.0] = 0 # sensitive covariate; \n",
    "                                # 0 is the protected class, 1 is the non-protected class\n",
    "    return X, y, x_sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part a)  [10 points]\n",
    "Using the logistic regression classifier provided by the python class fairness_demo (just like in the lecture notebook) calculate the accuracy and p%-rule ratio for all four datasets using the unconstrained classifier (i.e. no fairness constraints are imposed).\n",
    "\n",
    "# part b) [10 points]\n",
    "Note that for all four datasets the \"four-fifths rule\" is very much not satisfied. For each dataset impose the minimum fairness constraint such that the four-fiths rule is satisfied. What is the loss in accuracy for each dataset as compared to the unconstrained classifier performance?\n",
    "\n",
    "# part c) [5 points]\n",
    "Notice that (as least as far as the four datasets for $d=2,3,4, \\& \\;5$ are concerned) as the dimension $d$ increases  the following things happen:\n",
    "\n",
    "- the accuracy increases\n",
    "- the p%-rule ratio for the unconstrained classifier increases\n",
    "- the accuracy losses as calculated in part b decrease (at least approximately up to fluctuations)\n",
    "\n",
    "Look at the function get_dataset($d$) and consider how the generated dataset changes as a function of $d$. Do you expect the behavior described above to continue for all values of $d>5$? If so, explain why. If not, explain why not.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when d = 2\n",
      "accuracy =  75.6025\n",
      "p%-rule ratio =  26.794234903\n",
      "\n",
      "when d = 3\n",
      "accuracy =  80.7233333333\n",
      "p%-rule ratio =  33.6578812069\n",
      "\n",
      "when d = 4\n",
      "accuracy =  84.12375\n",
      "p%-rule ratio =  36.8799868285\n",
      "\n",
      "when d = 5\n",
      "accuracy =  86.947\n",
      "p%-rule ratio =  39.2738406591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_unconstrained(X,y,x_sensitive):\n",
    "    fd_unconstrained=fairness_demo()\n",
    "    w_unconstrained, p_rule_unconstrained, accuracy_unconstrained = fd_unconstrained.train(X,y,x_sensitive,-1.0)\n",
    "    print (\"accuracy = \", accuracy_unconstrained)\n",
    "    print (\"p%-rule ratio = \", p_rule_unconstrained)\n",
    "\n",
    "d = [2,3,4,5]\n",
    "\n",
    "for i in d:\n",
    "    print (\"when d =\", i)\n",
    "    train_unconstrained(get_dataset(i)[0],get_dataset(i)[1],get_dataset(i)[2])\n",
    "    print (\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when d = 2\n",
      "p%-rule ratio constrained = 94.3642038571\n",
      "accuracy constrained =  65.2675\n",
      "accuracy lose =  10.335\n",
      "\n",
      "when d = 3\n",
      "p%-rule ratio constrained = 93.5174069628\n",
      "accuracy constrained =  70.7333333333\n",
      "accuracy lose =  9.99\n",
      "\n",
      "when d = 4\n",
      "p%-rule ratio constrained = 91.3127257692\n",
      "accuracy constrained =  74.12125\n",
      "accuracy lose =  10.0025\n",
      "\n",
      "when d = 5\n",
      "p%-rule ratio constrained = 89.4580749414\n",
      "accuracy constrained =  76.831\n",
      "accuracy lose =  10.116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_constrained(X,y,x_sensitive,fairness_constraint):\n",
    "    \n",
    "    fd_unconstrained=fairness_demo()\n",
    "    w_unconstrained, p_rule_unconstrained, accuracy_unconstrained = fd_unconstrained.train(X,y,x_sensitive,-1.0)\n",
    "    \n",
    "    fd_constrained=fairness_demo()\n",
    "    w_constrained, p_rule_constrained, accuracy_constrained = fd_constrained.train(X,y,x_sensitive,fairness_constraint)\n",
    "    \n",
    "    print (\"p%-rule ratio constrained =\", p_rule_constrained)\n",
    "    print (\"accuracy constrained = \", accuracy_constrained)\n",
    "    print (\"accuracy lose = \", accuracy_unconstrained-accuracy_constrained)\n",
    "    \n",
    "for i in d:\n",
    "    print (\"when d =\", i)\n",
    "    train_constrained(get_dataset(i)[0],get_dataset(i)[1],get_dataset(i)[2],0.01)\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Answer:\n",
    "\n",
    "I think when d is more than 5, the three condition will keep the same but the speed of increase or decrease will be slow and the value will be similar. The reason is that the gap between protected value and unprotect value is getting smaller and smaller when dimension increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2  [10 points]\n",
    "\n",
    "Read the following review by Barocas and Selbst (or as much of the review as you find interesting):\n",
    "\n",
    "http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899\n",
    "\n",
    "Answer the following questions on the basis of what you've read.\n",
    "\n",
    "# part a)\n",
    "\n",
    "Consider the logic underlying the \"fairness aware\" classifier we explored in the previous problem. Consider the principle of nondiscrimination versus the principle of antisubordination. Which (if any) of the two principles is more in line with the approach taken by the algorithm? Why?\n",
    "\n",
    "# part b)\n",
    "\n",
    "Consider the \"fairness aware\" classifier we explored in the previous problem. In itself does it offer a solution to the problem of \"masking\" as described in the review?\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Answer: \n",
    "It is stated that nondiscrimination aims to eliminate the unfairness between individuals due to the decision makers choose in a protected group. On the contrary, anti-subordination theory preserve the principles in a core perspective that try to eliminate the individuals status on being a member of different groups. Both of them try to decrease the gap between the elimination of the individuals from their group features. Once the gap between the protected and the unprotected values become closer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Answer:\n",
    "\n",
    "It is stated that 'masks efforts to engage in intentional discrimination by abstracting to a level of analysis that fails to capture lower level variations that might otherwise make certain members of protected classes into more attractive candidates.' It shows us that masking provide to ignore lower level changes or variations in data. It tries to eliminate fluctuation in data. It also enables to gap between the protected and unprotected value discrimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3  [5 points]\n",
    "\n",
    "Consider the following illustration of a dataset in which the positive target labels are marked with plus signs, the green points constitute the non-protected class, and the blue points constitute the protected class. The distribution of the non-protected class is illustrated on the left, the distribution of the protected class is illustrated in the middle, and the graphic on the right shows the combined dataset.\n",
    "\n",
    "<img src=\"dataset.png\">\n",
    "\n",
    "Consider applying the \"fairness aware\" classifier in Problem 1 to the combined dataset, imposing fairness constraints such that the four-fifths rule is satisfied. Do you expect the loss of accuracy as you go from the unconstrained to the constrained classifier to be large or small? Why?\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "In the satisfaction of fairness or four-fifths rule and relte them with the given illustrated figure, it is obvious that majority and minority and overall population has different dynamics. In this perspective when thinking about the accuracy and constrained or unconstrained calssifiers, some biases in itself creates an accuracy problem. However loss of accuracy will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4  [15 points]\n",
    "#### (This is question 2.2 from Dunning's book)\n",
    "\n",
    "In a study of the effect of police presence on the incidence of crime, Di Tella and Schargrodsky (2004) write:\n",
    "\n",
    "“Following a terrorist attack on the main Jewish center in Buenos Aires, Argentina, in July\n",
    "1994, all Jewish institutions received police protection… Because the geographical\n",
    "distribution of these institutions can be presumed to be exogenous in a crime regression, this hideous event constitutes a natural experiment.”\n",
    "\n",
    "The authors find that blocks which were allocated extra police forces due to the presence of a Jewish institution experienced lower motor vehicle theft rates.  The control group consists of blocks in the same neighborhoods that do not have Jewish institutions.\n",
    "\n",
    "Answer the following three questions __in at least 6-10 sentences__.\n",
    "\n",
    "### part a) \n",
    "\n",
    "What do the authors mean by “presumed exogenous in a crime regression” and what is the relationship to as-if random assignment?  \n",
    "### part b) \n",
    "What are some potential threats to as-if random assignment?  [give at least two examples of potential threats]\n",
    "### part c) \n",
    "How might these threats be evaluated empirically?\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:\n",
    "\n",
    "Presumed Exogenous will preserve some royality interms of the data analysis period or creating th perior beliefs to the data. This adresses to the selecting data in a predefined condition and their affects on the categorization. In addition to that in a af -if scenerio there will be real selected group and another group for validating it. The aim will be creating relation between the studied and tested group. However there will be some error in validation process so as if scenerio has includes some biases in its nature. As a result, controled data can be indicator of assesign the succes of the data which is selected as a as-if method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5  [45 points]\n",
    "Consider the Titanic dataset below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the first three rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                            name     sex      age  sibsp  \\\n",
       "0       1         1   Allen, Miss. Elisabeth Walton  female  29.0000      0   \n",
       "1       1         1  Allison, Master. Hudson Trevor    male   0.9167      1   \n",
       "2       1         0    Allison, Miss. Helen Loraine  female   2.0000      1   \n",
       "\n",
       "   parch  ticket      fare    cabin embarked boat  body  \\\n",
       "0      0   24160  211.3375       B5        S    2   NaN   \n",
       "1      2  113781  151.5500  C22 C26        S   11   NaN   \n",
       "2      2  113781  151.5500  C22 C26        S  NaN   NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"https://serv.cusp.nyu.edu/classes/ML_2016_Spring/Bonus/titanic3.csv\");\n",
    "print(\"Here are the first three rows:\")\n",
    "data.iloc[0:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dictionary\n",
    "NAME: titanic3<br>\n",
    "SIZE: 1309 Passengers, 14 Variables<br><br>\n",
    "\n",
    "VARIABLE DESCRIPTIONS<br>\n",
    "Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) <br>\n",
    "survival: Survival (0 = No; 1 = Yes)<br>\n",
    "name: Name<br>\n",
    "sex: Sex<br>\n",
    "age: Age<br>\n",
    "sibsp: Number of Siblings/Spouses Aboard<br>\n",
    "parch: Number of Parents/Children Aboard<br>\n",
    "ticket: Ticket Number<br>\n",
    "fare: Passenger Fare (British pound)<br>\n",
    "cabin: Cabin<br>\n",
    "embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)<br>\n",
    "boat: Lifeboat<br>\n",
    "body: Body Identification Number<br>\n",
    "home.dest: Home/Destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part a) [30 points]\n",
    "\n",
    "Your goal is to train a classifier for the binary attribute “survival\" using age, sex, pclass, sibsp, and parch as features. You will do so using three different machine learning techniques:\n",
    "\n",
    "i) Naive Bayes Classification. [10 points]\n",
    "\n",
    "ii) Support Vector Machine. Try a linear SVM with soft margins as well as kernel SVM with polynomial and Gaussian kernels. Make sure to use a validation set to choose hyperparameters for each model where applicable. [10 points]\n",
    "\n",
    "iii) Random Forest Classification. [10 points]\n",
    "\n",
    "For each of the three models report out-of-sample accuracy--in order to do so, you will of course need to split the dataset into a training dataset and a test dataset.\n",
    "\n",
    "# part b)  [15 points]\n",
    "\n",
    "Repeat the exercise in part a, this time using cross validation. Report the mean accuracy for each model after doing 10 random splits of the data into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['age'])\n",
    "\n",
    "target = data['survived']\n",
    "\n",
    "X = data[['age', 'sex', 'pclass', 'sibsp', 'parch']]\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_test, target_train, target_test = train_test_split(X, target, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Naive Bayes = 0.837579617834\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classification\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, target_train)\n",
    "pred = gnb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy by Naive Bayes =\", 1.0*sum(target_test==pred)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by SVM with linear = 0.828025477707\n",
      "Accuracy by SVM with poly = 0.859872611465\n",
      "Accuracy by SVM with rbf = 0.837579617834\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, target_train)\n",
    "pred = svc.predict(X_test)\n",
    "print(\"Accuracy by SVM with linear =\", 1.0*sum(target_test==pred)/len(pred))\n",
    "\n",
    "svc = SVC(kernel='poly', max_iter=-1, tol=.01, degree=2)\n",
    "svc.fit(X_train, target_train)\n",
    "pred = svc.predict(X_test)\n",
    "print(\"Accuracy by SVM with poly =\", 1.0*sum(target_test==pred)/len(pred))\n",
    "\n",
    "svc = SVC(kernel='rbf')\n",
    "svc.fit(X_train, target_train)\n",
    "pred = svc.predict(X_test)\n",
    "print(\"Accuracy by SVM with rbf =\", 1.0*sum(target_test==pred)/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Random Forest = 0.792993630573\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=-1, n_estimators=1000)\n",
    "clf = clf.fit(X_train, target_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(\"Accuracy by Random Forest =\", 1.0*sum(target_test==pred)/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy of Naive Bayes = 0.783547779082\n",
      "Average Accuracy of SVM Linear = 0.778813346847\n",
      "Average Accuracy of SVM Poly = 0.804610761407\n",
      "Average Accuracy of SVM RBF = 0.781606031509\n",
      "Average Accuracy of Random Forest = 0.735707173086\n"
     ]
    }
   ],
   "source": [
    "for clf, name in [\n",
    "    (GaussianNB(), 'Naive Bayes'), \n",
    "    (SVC(kernel='linear'), 'SVM Linear'),\n",
    "    (SVC(kernel='poly', tol=.1, degree=2), 'SVM Poly'),\n",
    "    (SVC(kernel='rbf'), 'SVM RBF'),\n",
    "    (RandomForestClassifier(n_jobs=-1, n_estimators=500), 'Random Forest')\n",
    "]:\n",
    "    scores = cross_val_score(clf, X, target, cv=10, scoring='accuracy')\n",
    "    print(\"Average Accuracy of\", name, \"=\", scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
