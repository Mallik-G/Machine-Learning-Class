{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (20% credit)\n",
    "\n",
    "Assume you have a data set as below. It contains records of cars with three features: the type of the car (sports(1) or SUV(2)), the color of the car (red(1) or yellow(2)), and the origin of the car (domestic(1) or imported(2)). And the labels for the data are: stolen(1) and not(0). \n",
    "\n",
    "#### Questions:\n",
    "\n",
    "a) Calculate the following sample probabilities:\n",
    "P(Red|Stolen), P(SUV|Stolen), P(Domestic|Stolen), P(Red|Not Stolen) , P(SUV|Not Stolen), and P(Domestic|Not Stolen)\n",
    "\n",
    "b) Suggest a classification for a red, domestic SUV - whether it will be stolen or not - using Naive Bayes classifier. \n",
    "\n",
    "Please perform all the necessary computations \"by hands\" rather than using python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "import statsmodels.formula.api as smf\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import random\n",
    "#import shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=[1,0,1,0,1,0,1,0,0,1]\n",
    "X=[[1,1,1,2,2,2,2,2,1,1],[1,1,1,1,1,2,2,2,2,1],[1,1,1,1,2,2,2,1,2,2]]\n",
    "data=[y]+X\n",
    "data=pd.DataFrame(data).T\n",
    "data.columns=['Stolen?','Color','Type','Origin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stolen?</th>\n",
       "      <th>Color</th>\n",
       "      <th>Type</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stolen?  Color  Type  Origin\n",
       "0        1      1     1       1\n",
       "1        0      1     1       1\n",
       "2        1      1     1       1\n",
       "3        0      2     1       1\n",
       "4        1      2     1       2\n",
       "5        0      2     2       2\n",
       "6        1      2     2       2\n",
       "7        0      2     2       1\n",
       "8        0      1     2       2\n",
       "9        1      1     1       2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "### (a)\n",
    "\n",
    "P(Red|Stolen) = 3/5\n",
    "\n",
    "P(SUV|Stolen) = 1/5\n",
    "\n",
    "P(Domestic|Stolen) = 2/5\n",
    "\n",
    "P(Red|Not Stolen) = 2/5\n",
    "\n",
    "P(SUV|Not Stolen) = 3/5\n",
    "\n",
    "P(Domestic|Not Stolen) = 3/5\n",
    "\n",
    "\n",
    "### (b)\n",
    "\n",
    "p(stolen) = 3/5 * 1/5 * 2/5 * 1/2 = 3/125\n",
    "\n",
    "p(not stolen) = 2/5 * 3/5 * 3/5 * 1/2 = 9/125\n",
    "\n",
    "p(stolen) < p(not stolen)\n",
    "\n",
    "#### So it won't be stolen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (25% credit)\n",
    "Consider a following Guassian Naive Bayes problem.\n",
    "We use eight factors to predict if people have diabetes or not. The variabls are:\n",
    "\n",
    "y: The label (0 - no diabetes, 1 - diabetes)\n",
    "\n",
    "t_pre: Number of times pregnant\n",
    "\n",
    "glu: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "blood_p: Diastolic blood pressure (mm Hg)\n",
    "\n",
    "triceps: Triceps skin fold thickness (mm)\n",
    "\n",
    "serum: 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "b_m: Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "pedigree_f: Diabetes pedigree function\n",
    "\n",
    "age: Age (years)\n",
    "#### ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#### Questions:\n",
    "\n",
    "a)Train the classifier: use the training data to estimate prior probabilities $P(y=b)$ as well as the parameters (mean and standard deviation) of the sample distributions $P(x_i|y=b)$.\n",
    "\n",
    "b)Perform the classification for the test sample. \n",
    "\n",
    "c)Compare your result to y_test and report the classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"dia_train.csv\") \n",
    "y_train=data_train.iloc[:,1] \n",
    "X_train=data_train.iloc[:,2:] \n",
    "\n",
    "data_test=pd.read_csv(\"dia_test.csv\")\n",
    "y_test=data_test.iloc[:,1]\n",
    "X_test=data_test.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>t_pre</th>\n",
       "      <th>glu</th>\n",
       "      <th>blood_p</th>\n",
       "      <th>triceps</th>\n",
       "      <th>serum</th>\n",
       "      <th>b_m</th>\n",
       "      <th>pedigree_f</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>543</td>\n",
       "      <td>30.5</td>\n",
       "      <td>0.158</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.398</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>83</td>\n",
       "      <td>43.3</td>\n",
       "      <td>0.183</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y  t_pre  glu  blood_p  triceps  serum   b_m  pedigree_f  age\n",
       "0  0      1   89       66       23     94  28.1       0.167   21\n",
       "1  1      0  137       40       35    168  43.1       2.288   33\n",
       "2  1      2  197       70       45    543  30.5       0.158   53\n",
       "3  1      1  189       60       23    846  30.1       0.398   59\n",
       "4  0      1  103       30       38     83  43.3       0.183   33"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([y_train,X_train],axis=1)\n",
    "test = pd.concat([y_test,X_test],axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>t_pre</th>\n",
       "      <th>glu</th>\n",
       "      <th>blood_p</th>\n",
       "      <th>triceps</th>\n",
       "      <th>serum</th>\n",
       "      <th>b_m</th>\n",
       "      <th>pedigree_f</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mu1</th>\n",
       "      <td>0.699153</td>\n",
       "      <td>2.69091</td>\n",
       "      <td>111.467</td>\n",
       "      <td>69.2061</td>\n",
       "      <td>27.2</td>\n",
       "      <td>127.006</td>\n",
       "      <td>31.7091</td>\n",
       "      <td>0.468685</td>\n",
       "      <td>28.3939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.61006</td>\n",
       "      <td>24.6919</td>\n",
       "      <td>11.7133</td>\n",
       "      <td>10.4369</td>\n",
       "      <td>91.4861</td>\n",
       "      <td>6.33761</td>\n",
       "      <td>0.29175</td>\n",
       "      <td>8.53736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu2</th>\n",
       "      <td>0.300847</td>\n",
       "      <td>4.07042</td>\n",
       "      <td>144.141</td>\n",
       "      <td>74.5634</td>\n",
       "      <td>33.4789</td>\n",
       "      <td>209.211</td>\n",
       "      <td>35.2239</td>\n",
       "      <td>0.639042</td>\n",
       "      <td>35.7887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sigma2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.51862</td>\n",
       "      <td>30.6265</td>\n",
       "      <td>13.7993</td>\n",
       "      <td>9.7627</td>\n",
       "      <td>126.921</td>\n",
       "      <td>6.25849</td>\n",
       "      <td>0.439042</td>\n",
       "      <td>10.2635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               y    t_pre      glu  blood_p  triceps    serum      b_m  \\\n",
       "mu1     0.699153  2.69091  111.467  69.2061     27.2  127.006  31.7091   \n",
       "sigma1       NaN  2.61006  24.6919  11.7133  10.4369  91.4861  6.33761   \n",
       "mu2     0.300847  4.07042  144.141  74.5634  33.4789  209.211  35.2239   \n",
       "sigma2       NaN  3.51862  30.6265  13.7993   9.7627  126.921  6.25849   \n",
       "\n",
       "       pedigree_f      age  \n",
       "mu1      0.468685  28.3939  \n",
       "sigma1    0.29175  8.53736  \n",
       "mu2      0.639042  35.7887  \n",
       "sigma2   0.439042  10.2635  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### (a)\n",
    "\n",
    "def trainNaiveBayes(trainData):\n",
    "  #training Gausian Naive Bayes Classifier\n",
    "  tY=trainData.loc[:,trainData.columns[0]]\n",
    "  ind1=tY==0\n",
    "  ind2=tY==1\n",
    "  dp=pd.DataFrame(columns=trainData.columns, index=['mu1','sigma1','mu2','sigma2'])\n",
    "  #estimate priors\n",
    "  dp[trainData.columns[0]]['mu1']=1.0*sum(ind1)/len(trainData.index)\n",
    "  dp[trainData.columns[0]]['mu2']=1.0*sum(ind2)/len(trainData.index)\n",
    "  #estimate sample distribution paramters for p(xi|y=b)\n",
    "  for i in trainData.columns[1:]:\n",
    "    dp.loc['mu1',i]=(trainData[i][ind1]).mean()\n",
    "    dp.loc['sigma1',i]=(trainData[i][ind1]).std()\n",
    "    dp.loc['mu2',i]=(trainData[i][ind2]).mean()\n",
    "    dp.loc['sigma2',i]=(trainData[i][ind2]).std()\n",
    "  return dp\n",
    "\n",
    "dp=trainNaiveBayes(train)\n",
    "\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### (b)\n",
    "\n",
    "def classifyNaiveBayes(classData,dp):\n",
    "  #classifying using trained Gausian Naive Bayes Classifier\n",
    "  Y=classData.loc[:,classData.columns[0]]*0\n",
    "  for j in classData.index:\n",
    "    #start from the priors\n",
    "    P1=dp[classData.columns[0]]['mu1'];\n",
    "    P2=dp[classData.columns[0]]['mu2'];\n",
    "    #multiply by conditional probability densities p(xi|y=b)\n",
    "    for i in classData.columns[1:]:\n",
    "        if dp[i]['sigma1']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P1=P1*stats.norm.pdf(classData[i][j], loc=dp[i]['mu1'],scale=dp[i]['sigma1'])\n",
    "        \n",
    "        if dp[i]['sigma2']==0: #if sigma can not be defined (sample does not have variance)\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=1) #pick up arbitrary sigma if undefined\n",
    "        else:\n",
    "            P2=P2*stats.norm.pdf(classData[i][j], loc=dp[i]['mu2'],scale=dp[i]['sigma2']) \n",
    "    Y[j]=int(P2>P1)\n",
    " \n",
    "\n",
    "  return Y\n",
    "\n",
    "C=classifyNaiveBayes(test,dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.11392405063292"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### (c)\n",
    "\n",
    "#classification accuracy (over test set)\n",
    "100.0*sum(C==y_test)/len(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.27118644067797"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also report accuracy of the classifier over the training - it is slightly higher, although not that higher\n",
    "Ct=classifyNaiveBayes(train,dp)\n",
    "100.0*sum(Ct==y_train)/len(Ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (Credit 25%)\n",
    "We have an artificial data set split, while the training set contains both - labeled (Label_train) and unlabeled (Unlabel) data. Column 'y' is the label, and columns '0','1','2' are categorical variables.\n",
    "\n",
    "#### Questions:\n",
    "\n",
    "a) Use the labeled part data_train to predict the labels of X_Label_test, and report the classification accuracy.\n",
    "\n",
    "b) Improve the classification by using the unlabeled data data_Unlabel and the EM algorithm to predict labels of X_Label_test, and report the new accuracy by EM semi-supervised algorithm (use the same convergence criteria as in the lecture notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train=pd.read_csv(\"EM_train.csv\")\n",
    "y_Label_train=data_train.iloc[:,1] \n",
    "X_Label_train=data_train.iloc[:,2:] \n",
    "\n",
    "data_test=pd.read_csv(\"EM_test.csv\")\n",
    "y_Label_test=data_test.iloc[:,1]\n",
    "X_Label_test=data_test.iloc[:,2:]\n",
    "\n",
    "data_Unlabel=pd.read_csv(\"EM_Unlabel.csv\")\n",
    "X_Unlabel=data_Unlabel.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([y_Label_train,X_Label_train],axis=1)\n",
    "test = pd.concat([y_Label_test,X_Label_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[          y    0    1    2\n",
       " 1  0.555556    0  0.2    0\n",
       " 2       NaN  0.2    0  0.2\n",
       " 3       NaN  0.2    0    0\n",
       " 4       NaN  0.2  0.2  0.2\n",
       " 5       NaN    0    0    0\n",
       " 6       NaN  0.4  0.6  0.6,           y     0     1     2\n",
       " 1  0.444444   0.5  0.25  0.25\n",
       " 2       NaN  0.25   0.5   0.5\n",
       " 3       NaN     0  0.25     0\n",
       " 4       NaN     0     0     0\n",
       " 5       NaN  0.25     0     0\n",
       " 6       NaN     0     0  0.25]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### (a)\n",
    "\n",
    "def trainNaiveBayesDiscrete(trainData):\n",
    "  #training discrete Naive Bayes Classifier\n",
    "  tY=trainData.loc[:,trainData.columns[0]]\n",
    "  m=max([trainData[j][i] for j in trainData.columns[1:] for i in trainData.index]) #maximal number of classes in each feature of a training set\n",
    "  #create output data structure for the probabilities - same column labels, rows correspond to values of x and there are two arrays like that for different b\n",
    "  dp=[pd.DataFrame(columns=trainData.columns, index=range(1,m+1)), pd.DataFrame(columns=trainData.columns, index=range(1,m+1))]\n",
    "  #split the training data between two labels\n",
    "  ind1=tY==0\n",
    "  ind2=tY==1\n",
    "  #estimate P(y=b)  \n",
    "  dp[0][trainData.columns[0]][1]=1.0*ind1.sum()/len(trainData.index)\n",
    "  dp[1][trainData.columns[0]][1]=1.0*ind2.sum()/len(trainData.index)\n",
    "  #estimate conditional probabilities P(x|y=b)\n",
    "  for j in trainData.columns[1:]:\n",
    "    for i in range(1,m+1):\n",
    "        dp[0].loc[i,j]=1.0*(trainData[j][ind1]==i).sum()/ind1.sum();\n",
    "        dp[1].loc[i,j]=1.0*(trainData[j][ind2]==i).sum()/ind2.sum();\n",
    "  return dp\n",
    "\n",
    "dp=trainNaiveBayesDiscrete(train)\n",
    "dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y    0    1    2\n",
       "1  0.555556    0  0.2    0\n",
       "2       NaN  0.2    0  0.2\n",
       "3       NaN  0.2    0    0\n",
       "4       NaN  0.2  0.2  0.2\n",
       "5       NaN    0    0    0\n",
       "6       NaN  0.4  0.6  0.6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We correctly classified 63.8888888889 percents of the trips based on the labeled data only\n"
     ]
    }
   ],
   "source": [
    "def classifyNaiveBayesDiscrete(classData,dp):\n",
    "  #classifying using trained discrete Naive Bayes Classifier\n",
    "  Y=classData[classData.columns[0]]*0 #initialize the empty array \n",
    "  for i in classData.index: #for al records to classify\n",
    "    #start with the priors\n",
    "    P1=dp[0][classData.columns[0]][1]; \n",
    "    P2=dp[1][classData.columns[0]][1];\n",
    "    #and multiply them by the corresponding conditional probabilities P(x_i|y=b)\n",
    "    for j in classData.columns[1:]:\n",
    "      P1=P1*dp[0][j][classData[j][i]]\n",
    "      P2=P2*dp[1][j][classData[j][i]]\n",
    "    Y[i]=int(P2>P1) #finally for each record decide which P(y|x) is higher and choose the label\n",
    "  return Y\n",
    "\n",
    "C=classifyNaiveBayesDiscrete(test,dp)\n",
    "\n",
    "acc=format(100.0*sum(C==y_Label_test)/len(y_Label_test))\n",
    "print \"We correctly classified {0} percents of the trips based on the labeled data only\".format(acc)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -590.452072842\n",
      "Iteration 2: log maximum liklihood = -572.346722303\n",
      "Iteration 3: log maximum liklihood = -559.190769042\n",
      "Iteration 4: log maximum liklihood = -556.633109435\n",
      "Iteration 5: log maximum liklihood = -556.067798435\n",
      "After EM we correctly classified 94.4444444444 percents of the trips\n"
     ]
    }
   ],
   "source": [
    "### (b)\n",
    "\n",
    "#implementation of Expectation-Maximization algorithm for partially labeled data\n",
    "def EM(X_Label,y_Label,X_Unlabel,dp):\n",
    "  t = 0  \n",
    "  haslabels=len(y_Label)>0\n",
    "\n",
    "  while True:\n",
    "    t = t + 1\n",
    "\n",
    "    classData=X_Unlabel\n",
    "    # Now we want to calculate P(y=1|x) and P(y=2|x) for all observations xj. (these are bunch of scalars)\n",
    "    # we need this to calculate new dp. Basically speaking, for every new iteration we need a new dp.\n",
    "\n",
    "    #for y=1 and y=2\n",
    "\n",
    "    p_x_1=[] #unnormalized P(y=1|x)\n",
    "    p_x_2=[] #unnormalized P(y=2|x)\n",
    "    cols=dp[0].columns\n",
    "\n",
    "    for i in classData.index:\n",
    "        P1=dp[0][cols[0]][1];\n",
    "        P2=dp[1][cols[0]][1];\n",
    "        for j in classData.columns:\n",
    "            P1=P1*dp[0][j][classData[j][i]]\n",
    "            P2=P2*dp[1][j][classData[j][i]]\n",
    "        p_x_1.append(P1)\n",
    "        p_x_2.append(P2)\n",
    "\n",
    "    #Rescale p_x_1 and p_x_2:\n",
    "    summ=np.asarray(p_x_1)+np.asarray(p_x_2)\n",
    "    p_x_1_s=np.asarray(p_x_1)/summ\n",
    "    p_x_2_s=np.asarray(p_x_2)/summ\n",
    "    inds_1 = np.where(np.isnan(p_x_1_s))\n",
    "    inds_2 = np.where(np.isnan(p_x_2_s))\n",
    "    p_x_1_s[inds_1]=0.5\n",
    "    p_x_2_s[inds_2]=0.5\n",
    "    #Now let's calculate P(y=1) and P(y=2)\n",
    "    p_1=p_x_1_s.sum()/len(p_x_1_s)\n",
    "    p_2=p_x_2_s.sum()/len(p_x_2_s)\n",
    "\n",
    "\n",
    "    #Now let's calculate the probability distribution of P(xi|y=1) and P(xi|y=2)\n",
    "    \n",
    "    m=max([classData[j][i] for j in classData.columns for i in classData.index]) #maximal number of classes in each feature of a training set\n",
    "\n",
    "    #create output data structure for the probabilities - new iteration\n",
    "    \n",
    "    dp1=[pd.DataFrame(columns=cols, index=range(1,m+1)), pd.DataFrame(columns=cols, index=range(1,m+1))]\n",
    "\n",
    "    #P(y=b)  \n",
    "    dp1[0][cols[0]][1]=p_1\n",
    "    dp1[1][cols[0]][1]=p_2\n",
    "\n",
    "\n",
    "    #estimate conditional probabilities P(x|y=b) -do we add labeled data to fit?\n",
    "\n",
    "    temp=np.concatenate((np.asmatrix(X_Unlabel),np.asarray(pd.DataFrame(p_x_1_s)),np.asarray(pd.DataFrame(p_x_2_s))), axis=1)\n",
    "    temp=pd.DataFrame(temp)\n",
    "    if haslabels:\n",
    "        temp_l=np.concatenate((np.asmatrix(X_Label),np.asmatrix(1*(y_Label==0)).transpose(),np.asmatrix(1*(y_Label==1)).transpose()),axis=1)\n",
    "        temp_l=pd.DataFrame(temp_l)\n",
    "        pd.concat([temp,temp_l])\n",
    "   \n",
    "\n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        for i in range(len(dp[0])):\n",
    "\n",
    "            dp1[0].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-2].sum()/temp.iloc[:,-2].sum()\n",
    "            dp1[1].iloc[i,j]=temp[temp.iloc[:,j-1]==i+1].iloc[:,-1].sum()/temp.iloc[:,-1].sum()\n",
    "\n",
    "        ############################################################################################\n",
    "    # Now we use dp to decide whether to continue our iterations\n",
    "    \n",
    "    if (sum(sum((dp1[0]-dp[0])**2))+sum(sum((dp1[1]-dp[1])**2)))<0.001: #if dp does not change much\n",
    "        break\n",
    "    else: \n",
    "        dp=dp1  #save new dp and perform next iteration\n",
    "\n",
    "        \n",
    "    ###############################################################################################\n",
    "        #Calculate the log-likelihood\n",
    "        \n",
    "        L=0\n",
    "        \n",
    "        for i in classData.index:\n",
    "            P1=dp[0][cols[0]][1];\n",
    "            P2=dp[1][cols[0]][1];\n",
    "            for j in classData.columns:\n",
    "                P1=P1*dp[0][j][classData[j][i]]\n",
    "                P2=P2*dp[1][j][classData[j][i]]\n",
    "            temp=math.log(P1+P2)\n",
    "            L=L+temp\n",
    "        if haslabels:    \n",
    "          for i in X_Label.index:\n",
    "            yi=y_Label[i]\n",
    "            P=dp[yi][cols[0]][1];\n",
    "            for j in X_Label.columns:\n",
    "                P=P*dp[yi][j][X_Label[j][i]]\n",
    "            L=L+math.log(P)\n",
    "        \n",
    "        print \"Iteration {0}: log maximum liklihood = {1}\".format(t,L)    \n",
    "        \n",
    "        \n",
    "  return dp\n",
    "\n",
    "#perform EM estimation for theta\n",
    "dpEM=EM(X_Label_train,y_Label_train,X_Unlabel,dp)\n",
    "#OS test\n",
    "C=classifyNaiveBayesDiscrete(test,dpEM) #classify test data with a new theta given by EM\n",
    "acc=100.0*sum(C==y_Label_test)/len(y_Label_test)\n",
    "print \"After EM we correctly classified {0} percents of the trips\".format(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (Credit 30%)\n",
    "For the similar artifitial data uploaded below:\n",
    "\n",
    "#### Question: \n",
    "\n",
    "a) Apply the EM algorithm (no observed labels, random initial choice of $\\theta$) for clustering the data records into two clusters. Report your result (a vector of cluster numbers for each data record). \n",
    "\n",
    "b) Repeat the clustering 10 times with different random choices of $\\theta$ and analyze the stability of the clustering (matching labels accross different clusterings (use the choice of 0 and 1 labels best matching the previous clustering), estimate average label and its standard error for each record)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"EM_Cluster.csv\")\n",
    "X=data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  5  6  6\n",
       "1  4  6  6\n",
       "2  1  5  5\n",
       "3  1  3  4\n",
       "4  5  6  2"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -550.154324849\n",
      "Iteration 2: log maximum liklihood = -546.993947751\n",
      "Iteration 3: log maximum liklihood = -543.900022896\n",
      "Iteration 4: log maximum liklihood = -540.926855276\n",
      "Iteration 5: log maximum liklihood = -538.942993536\n",
      "Iteration 6: log maximum liklihood = -537.948294685\n",
      "Iteration 7: log maximum liklihood = -537.307000599\n",
      "Iteration 8: log maximum liklihood = -536.627131653\n",
      "Iteration 9: log maximum liklihood = -535.699541814\n",
      "Iteration 10: log maximum liklihood = -534.351403321\n",
      "Iteration 11: log maximum liklihood = -532.420831855\n",
      "Iteration 12: log maximum liklihood = -529.641212864\n",
      "Iteration 13: log maximum liklihood = -525.370333094\n",
      "Iteration 14: log maximum liklihood = -519.393160528\n",
      "Iteration 15: log maximum liklihood = -512.961265175\n",
      "Iteration 16: log maximum liklihood = -508.411538536\n",
      "Iteration 17: log maximum liklihood = -506.972542264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[          y            0            1           2\n",
       " 1  0.499138      0.40239     0.253141    0.381396\n",
       " 2       NaN     0.179784        0.467     0.29745\n",
       " 3       NaN      0.21471     0.193681     0.18712\n",
       " 4       NaN     0.150241   0.00845112   0.0445784\n",
       " 5       NaN    0.0528737    0.0776716   0.0850116\n",
       " 6       NaN  6.24274e-09  5.50834e-05  0.00444371,\n",
       "           y          0           1           2\n",
       " 1  0.500862  0.0553506   0.0329525  0.00021356\n",
       " 2       NaN  0.0490127  0.00997817   0.0838703\n",
       " 3       NaN  0.0712514   0.0351639   0.0797316\n",
       " 4       NaN   0.287617    0.276801    0.183753\n",
       " 5       NaN    0.30859    0.245848    0.314593\n",
       " 6       NaN   0.228178    0.399257    0.337839]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### (a)\n",
    "\n",
    "# np.random.seed(2016)\n",
    "#initialize theta randomly\n",
    "dp[0].iloc[0,0]=np.random.uniform(0,1)\n",
    "dp[1].iloc[0,0]=dp[1].iloc[0,0]=1-dp[0].iloc[0,0]\n",
    "for j in range(1,len(dp[0].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[0]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[0].iloc[:,j]=b\n",
    "for j in range(1,len(dp[1].T)):\n",
    "    b=np.random.uniform(0,1,len(dp[1]))\n",
    "    b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "    dp[1].iloc[:,j]=b\n",
    "    \n",
    "dpEM=EM([],[],X,dp)\n",
    "\n",
    "dpEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log maximum liklihood = -553.265915479\n",
      "Iteration 2: log maximum liklihood = -548.358768562\n",
      "Iteration 3: log maximum liklihood = -544.629912759\n",
      "Iteration 4: log maximum liklihood = -542.034504784\n",
      "Iteration 5: log maximum liklihood = -540.497706012\n",
      "Iteration 6: log maximum liklihood = -539.52502777\n",
      "Iteration 7: log maximum liklihood = -538.859354936\n",
      "Iteration 8: log maximum liklihood = -538.440146378\n",
      "time 1\n",
      "[         y          0          1          2\n",
      "1  0.63895   0.256017   0.153824   0.288186\n",
      "2      NaN   0.159325   0.343516   0.222033\n",
      "3      NaN   0.114498  0.0674352  0.0193632\n",
      "4      NaN   0.340716  0.0530326   0.116631\n",
      "5      NaN  0.0806494   0.253348   0.254187\n",
      "6      NaN  0.0487947   0.128843  0.0995997,          y           0            1          2\n",
      "1  0.36105    0.180001     0.123448  0.0175598\n",
      "2      NaN     0.03458    0.0515312   0.134629\n",
      "3      NaN    0.193045     0.197197   0.335026\n",
      "4      NaN  0.00373057      0.30182   0.110135\n",
      "5      NaN    0.358459  7.67473e-05   0.104105\n",
      "6      NaN    0.230185     0.325927   0.298544]\n",
      "Iteration 1: log maximum liklihood = -553.26855695\n",
      "Iteration 2: log maximum liklihood = -549.971124078\n",
      "Iteration 3: log maximum liklihood = -545.091518768\n",
      "Iteration 4: log maximum liklihood = -538.236009468\n",
      "Iteration 5: log maximum liklihood = -530.205175533\n",
      "Iteration 6: log maximum liklihood = -523.314106838\n",
      "Iteration 7: log maximum liklihood = -518.908284696\n",
      "Iteration 8: log maximum liklihood = -514.854960894\n",
      "Iteration 9: log maximum liklihood = -510.885726832\n",
      "Iteration 10: log maximum liklihood = -508.27836239\n",
      "Iteration 11: log maximum liklihood = -507.184749091\n",
      "time 2\n",
      "[          y          0            1            2\n",
      "1  0.462067   0.427832     0.262974      0.41015\n",
      "2       NaN   0.187823     0.453013     0.309534\n",
      "3       NaN   0.224071     0.210828     0.193244\n",
      "4       NaN   0.117467   0.00172128    0.0418932\n",
      "5       NaN  0.0428067    0.0714636    0.0451772\n",
      "6       NaN  1.603e-15  5.09565e-09  1.58583e-06,           y          0          1           2\n",
      "1  0.537933  0.0574135  0.0396807  0.00178343\n",
      "2       NaN  0.0511195   0.053488   0.0882098\n",
      "3       NaN  0.0730969  0.0313594   0.0818723\n",
      "4       NaN   0.306302   0.264088    0.176468\n",
      "5       NaN   0.299615   0.239591    0.332988\n",
      "6       NaN   0.212453   0.371793    0.318679]\n",
      "Iteration 1: log maximum liklihood = -546.637719541\n",
      "Iteration 2: log maximum liklihood = -539.841600466\n",
      "Iteration 3: log maximum liklihood = -532.57516843\n",
      "Iteration 4: log maximum liklihood = -526.717581298\n",
      "Iteration 5: log maximum liklihood = -523.037763422\n",
      "Iteration 6: log maximum liklihood = -518.442907752\n",
      "Iteration 7: log maximum liklihood = -512.548809772\n",
      "Iteration 8: log maximum liklihood = -508.002713353\n",
      "Iteration 9: log maximum liklihood = -505.869279505\n",
      "time 3\n",
      "[          y          0          1           2\n",
      "1  0.514985  0.0478841   0.041662  0.00873701\n",
      "2       NaN  0.0484046  0.0687706   0.0836838\n",
      "3       NaN  0.0436478   0.022768   0.0459216\n",
      "4       NaN   0.340547   0.222979    0.184315\n",
      "5       NaN   0.297596   0.256199     0.34583\n",
      "6       NaN    0.22192   0.387621    0.331513,           y           0           1           2\n",
      "1  0.485015    0.420424    0.250305    0.383445\n",
      "2       NaN    0.184238    0.417883    0.303868\n",
      "3       NaN    0.248197    0.211459    0.226146\n",
      "4       NaN   0.0900403   0.0577841   0.0399291\n",
      "5       NaN   0.0571014   0.0617838   0.0451593\n",
      "6       NaN  4.0642e-11  0.00078546  0.00145228]\n",
      "Iteration 1: log maximum liklihood = -557.735876017\n",
      "Iteration 2: log maximum liklihood = -553.924450279\n",
      "Iteration 3: log maximum liklihood = -551.753722521\n",
      "Iteration 4: log maximum liklihood = -548.550581103\n",
      "Iteration 5: log maximum liklihood = -543.233883232\n",
      "Iteration 6: log maximum liklihood = -536.024368141\n",
      "Iteration 7: log maximum liklihood = -528.363562388\n",
      "Iteration 8: log maximum liklihood = -520.827008988\n",
      "Iteration 9: log maximum liklihood = -513.403689541\n",
      "Iteration 10: log maximum liklihood = -508.54737455\n",
      "Iteration 11: log maximum liklihood = -507.03450703\n",
      "time 4\n",
      "[          y          0           1            2\n",
      "1  0.493555   0.050945   0.0333891  0.000163668\n",
      "2       NaN  0.0466188  0.00903555     0.084649\n",
      "3       NaN  0.0717233   0.0319031    0.0790947\n",
      "4       NaN   0.287153    0.280738     0.178988\n",
      "5       NaN   0.312004    0.240236     0.314662\n",
      "6       NaN   0.231556    0.404698     0.342443,           y            0            1           2\n",
      "1  0.506445     0.401677     0.249539    0.375945\n",
      "2       NaN      0.18023     0.461325     0.29361\n",
      "3       NaN      0.21218     0.194572    0.186192\n",
      "4       NaN     0.152676    0.0084855   0.0512306\n",
      "5       NaN    0.0532362    0.0855668   0.0882561\n",
      "6       NaN  4.84112e-07  0.000512357  0.00476714]\n",
      "Iteration 1: log maximum liklihood = -551.594765953\n",
      "Iteration 2: log maximum liklihood = -544.177430326\n",
      "Iteration 3: log maximum liklihood = -530.618006874\n",
      "Iteration 4: log maximum liklihood = -515.56435886\n",
      "Iteration 5: log maximum liklihood = -508.045651849\n",
      "Iteration 6: log maximum liklihood = -505.802609716\n",
      "Iteration 7: log maximum liklihood = -504.826927722\n",
      "time 5\n",
      "[          y            0           1          2\n",
      "1  0.510462     0.391705     0.25934    0.37314\n",
      "2       NaN     0.198938    0.402089   0.303496\n",
      "3       NaN     0.252064    0.204439    0.21904\n",
      "4       NaN     0.095136   0.0549955  0.0527222\n",
      "5       NaN    0.0621517     0.07438  0.0330093\n",
      "6       NaN  5.68695e-06  0.00475659  0.0185925,           y          0          1           2\n",
      "1  0.489538  0.0584653  0.0213957  5.2002e-06\n",
      "2       NaN  0.0260158  0.0670922   0.0726262\n",
      "3       NaN  0.0289828  0.0202795   0.0439631\n",
      "4       NaN   0.348255   0.234474     0.17848\n",
      "5       NaN   0.304831    0.25317    0.374128\n",
      "6       NaN    0.23345   0.403588    0.330797]\n",
      "Iteration 1: log maximum liklihood = -543.984952461\n",
      "Iteration 2: log maximum liklihood = -534.660871468\n",
      "Iteration 3: log maximum liklihood = -525.352037836\n",
      "Iteration 4: log maximum liklihood = -515.91654383\n",
      "Iteration 5: log maximum liklihood = -509.212948876\n",
      "Iteration 6: log maximum liklihood = -506.72684653\n",
      "Iteration 7: log maximum liklihood = -505.902109136\n",
      "time 6\n",
      "[         y            0            1           2\n",
      "1  0.52354     0.393062     0.245908    0.363778\n",
      "2      NaN     0.179566     0.450232    0.294411\n",
      "3      NaN     0.222559     0.194216    0.198004\n",
      "4      NaN     0.147257    0.0317036   0.0459525\n",
      "5      NaN    0.0575555    0.0773327    0.090971\n",
      "6      NaN  2.04722e-07  0.000606704  0.00688371,          y          0           1            2\n",
      "1  0.47646  0.0478278   0.0296234  5.01117e-05\n",
      "2      NaN   0.042555  0.00499657     0.076272\n",
      "3      NaN  0.0552797   0.0264572    0.0622725\n",
      "4      NaN   0.297931    0.264994     0.189371\n",
      "5      NaN   0.316542    0.254833     0.319802\n",
      "6      NaN   0.239864    0.419095     0.352232]\n",
      "Iteration 1: log maximum liklihood = -555.061844436\n",
      "Iteration 2: log maximum liklihood = -553.464158178\n",
      "Iteration 3: log maximum liklihood = -550.993206524\n",
      "Iteration 4: log maximum liklihood = -547.63413221\n",
      "Iteration 5: log maximum liklihood = -544.463336325\n",
      "Iteration 6: log maximum liklihood = -541.776640892\n",
      "Iteration 7: log maximum liklihood = -538.60004731\n",
      "Iteration 8: log maximum liklihood = -534.597844947\n",
      "Iteration 9: log maximum liklihood = -530.025456292\n",
      "Iteration 10: log maximum liklihood = -524.288875388\n",
      "Iteration 11: log maximum liklihood = -517.396728328\n",
      "Iteration 12: log maximum liklihood = -511.402647786\n",
      "Iteration 13: log maximum liklihood = -507.629921597\n",
      "Iteration 14: log maximum liklihood = -505.822320326\n",
      "Iteration 15: log maximum liklihood = -505.10896407\n",
      "time 7\n",
      "[          y          0            1           2\n",
      "1  0.440105  0.0377704    0.0120984  2.0106e-09\n",
      "2       NaN   0.033937  0.000269738   0.0700722\n",
      "3       NaN  0.0277594    0.0234768   0.0435613\n",
      "4       NaN   0.314782     0.245224    0.199983\n",
      "5       NaN   0.326073     0.268207    0.319854\n",
      "6       NaN   0.259678     0.450724    0.366529,           y            0           1          2\n",
      "1  0.559895      0.37855     0.24564     0.3402\n",
      "2       NaN     0.177444    0.425038   0.285119\n",
      "3       NaN      0.23333    0.185666   0.203898\n",
      "4       NaN     0.143795   0.0623919  0.0469234\n",
      "5       NaN    0.0668807   0.0783458   0.105789\n",
      "6       NaN  4.09498e-07  0.00291909  0.0180699]\n",
      "Iteration 1: log maximum liklihood = -538.778157906\n",
      "Iteration 2: log maximum liklihood = -519.843522996\n",
      "Iteration 3: log maximum liklihood = -509.506180339\n",
      "Iteration 4: log maximum liklihood = -506.505468199\n",
      "Iteration 5: log maximum liklihood = -505.256669497\n",
      "time 8\n",
      "[          y            0          1          2\n",
      "1  0.518949     0.378486   0.265701   0.365679\n",
      "2       NaN     0.189091   0.375889   0.293479\n",
      "3       NaN     0.261548   0.207534   0.236525\n",
      "4       NaN    0.0766244  0.0769114  0.0415392\n",
      "5       NaN    0.0942407  0.0630731   0.030238\n",
      "6       NaN  1.03432e-05  0.0108917  0.0325402,           y          0          1           2\n",
      "1  0.481051  0.0668463   0.010336  0.00147102\n",
      "2       NaN  0.0335873  0.0894457   0.0793586\n",
      "3       NaN  0.0148162  0.0136914   0.0220125\n",
      "4       NaN   0.372691   0.213998    0.192763\n",
      "5       NaN   0.274495   0.268522    0.383136\n",
      "6       NaN   0.237564   0.404006    0.321259]\n",
      "Iteration 1: log maximum liklihood = -543.355243243\n",
      "Iteration 2: log maximum liklihood = -538.137217965\n",
      "Iteration 3: log maximum liklihood = -534.72877809\n",
      "Iteration 4: log maximum liklihood = -532.041731489\n",
      "Iteration 5: log maximum liklihood = -529.086700218\n",
      "Iteration 6: log maximum liklihood = -525.085636816\n",
      "Iteration 7: log maximum liklihood = -519.763387125\n",
      "Iteration 8: log maximum liklihood = -513.674005137\n",
      "Iteration 9: log maximum liklihood = -509.430892412\n",
      "Iteration 10: log maximum liklihood = -507.721905134\n",
      "Iteration 11: log maximum liklihood = -506.670318849\n",
      "Iteration 12: log maximum liklihood = -505.734182993\n",
      "Iteration 13: log maximum liklihood = -505.022392628\n",
      "time 9\n",
      "[          y          0          1            2\n",
      "1  0.530033  0.0893583  0.0265474  4.40887e-05\n",
      "2       NaN  0.0387747  0.0963657    0.0741238\n",
      "3       NaN  0.0291176  0.0296191    0.0448597\n",
      "4       NaN   0.335006   0.220182      0.19586\n",
      "5       NaN   0.292123   0.249951     0.374778\n",
      "6       NaN    0.21562   0.377335     0.310334,           y            0            1           2\n",
      "1  0.469967     0.385577     0.274033    0.405247\n",
      "2       NaN     0.199448     0.397939      0.3217\n",
      "3       NaN     0.271134     0.209774    0.233115\n",
      "4       NaN    0.0882682    0.0556495   0.0222849\n",
      "5       NaN    0.0555728    0.0626052  0.00288331\n",
      "6       NaN  1.51748e-21  5.25493e-09   0.0147699]\n",
      "Iteration 1: log maximum liklihood = -537.898255028\n",
      "Iteration 2: log maximum liklihood = -519.369124019\n",
      "Iteration 3: log maximum liklihood = -509.937385948\n",
      "Iteration 4: log maximum liklihood = -507.383440998\n",
      "Iteration 5: log maximum liklihood = -505.737770105\n",
      "Iteration 6: log maximum liklihood = -504.851911441\n",
      "time 10\n",
      "[          y          0          1            2\n",
      "1  0.509251  0.0714771  0.0233175  0.000437187\n",
      "2       NaN  0.0291066   0.103787    0.0938724\n",
      "3       NaN  0.0133919   0.019868    0.0367724\n",
      "4       NaN   0.367212   0.211155     0.179666\n",
      "5       NaN   0.294393   0.250618     0.381886\n",
      "6       NaN   0.224419   0.391255     0.307366,           y            0          1          2\n",
      "1  0.490749     0.391588   0.266903    0.38768\n",
      "2       NaN     0.202676   0.377467   0.290722\n",
      "3       NaN     0.277203   0.212263   0.233535\n",
      "4       NaN    0.0652968  0.0719845    0.04644\n",
      "5       NaN    0.0632352  0.0698467  0.0112571\n",
      "6       NaN  8.34694e-08  0.0015349  0.0303663]\n"
     ]
    }
   ],
   "source": [
    "### (b)\n",
    "\n",
    "estimate={}\n",
    "for i in range(10):\n",
    "    #initialize theta randomly\n",
    "    dp[0].iloc[0,0]=np.random.uniform(0,1)\n",
    "    dp[1].iloc[0,0]=dp[1].iloc[0,0]=1-dp[0].iloc[0,0]\n",
    "    for j in range(1,len(dp[0].T)):\n",
    "        b=np.random.uniform(0,1,len(dp[0]))\n",
    "        b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "        dp[0].iloc[:,j]=b     \n",
    "        \n",
    "    for j in range(1,len(dp[1].T)):\n",
    "        b=np.random.uniform(0,1,len(dp[1]))\n",
    "        b=np.asarray(b)/float(np.asarray(b).sum())\n",
    "        dp[1].iloc[:,j]=b\n",
    "            \n",
    "    dpEM=EM([],[],X,dp)\n",
    "    \n",
    "    \n",
    "    print \"time\",i+1\n",
    "    print dpEM\n",
    "    estimate[i]=dpEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average label for  each record\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[         y         0         1         2\n",
       " 1  0.51419  0.214454  0.132476  0.181031\n",
       " 2      NaN  0.111158  0.230297  0.182935\n",
       " 3      NaN  0.126038  0.101209  0.111639\n",
       " 4      NaN   0.24219  0.139864  0.123755\n",
       " 5      NaN  0.185959  0.180481  0.219059\n",
       " 6      NaN    0.1202  0.215673   0.18158,\n",
       "          y         0         1         2\n",
       " 1  0.48581  0.238837   0.15109  0.191339\n",
       " 2      NaN  0.113189  0.234621  0.194611\n",
       " 3      NaN  0.160726  0.130272  0.162803\n",
       " 4      NaN  0.186899  0.153567  0.105403\n",
       " 5      NaN  0.184997  0.137434   0.17675\n",
       " 6      NaN  0.115352  0.193016  0.169094]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### estimate average label and its standard error for each record\n",
    "\n",
    "estimate_mean = dp\n",
    "estimate_mean[0].iloc[0,0]=0\n",
    "estimate_mean[1].iloc[0,0]=0\n",
    "for j in range(1,len(dp[0].T)):\n",
    "    estimate_mean[0].iloc[:,j]=0\n",
    "for j in range(1,len(dp[1].T)):\n",
    "    estimate_mean[1].iloc[:,j]=0\n",
    "\n",
    "for i in range(10):\n",
    "    estimate_mean[0] += estimate[i][0]\n",
    "    estimate_mean[1] += estimate[i][1]\n",
    "    \n",
    "estimate_mean[0] = estimate_mean[0]/10\n",
    "estimate_mean[1] = estimate_mean[1]/10\n",
    "print \"average label for  each record\"\n",
    "estimate_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard error for  each record\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[          y         0         1         2\n",
       " 1  0.213931  0.229372  0.142577  0.278157\n",
       " 2       NaN  0.103107  0.289723  0.149744\n",
       " 3       NaN  0.130511  0.108952  0.110342\n",
       " 4       NaN  0.158897  0.120497   0.10126\n",
       " 5       NaN  0.145064  0.130196  0.167301\n",
       " 6       NaN  0.123226  0.216293  0.169227,\n",
       "           y         0         1         2\n",
       " 1  0.206994   0.19274  0.131999  0.196698\n",
       " 2       NaN  0.101219  0.217888  0.143115\n",
       " 3       NaN  0.129797  0.113025  0.130853\n",
       " 4       NaN  0.162051  0.141497  0.098487\n",
       " 5       NaN  0.174845  0.125543  0.195226\n",
       " 6       NaN  0.144227  0.283087  0.209618]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### standard error for each record\n",
    "\n",
    "estimate_std = dp\n",
    "estimate_std[0].iloc[0,0]=0\n",
    "estimate_std[1].iloc[0,0]=0\n",
    "for j in range(1,len(dp[0].T)):\n",
    "    estimate_std[0].iloc[:,j]=0\n",
    "for j in range(1,len(dp[1].T)):\n",
    "    estimate_std[1].iloc[:,j]=0\n",
    "    \n",
    "for i in range(10):\n",
    "    \n",
    "    estimate_std[0] += (estimate[i][0]-estimate_mean[0])**2\n",
    "    estimate_std[1] += (estimate[i][1]-estimate_mean[1])**2\n",
    "\n",
    "estimate_std[0] = estimate_std[0]/10\n",
    "estimate_std[1] = estimate_std[1]/10\n",
    "\n",
    "estimate_std[0].iloc[0,0]=sqrt(estimate_std[0].iloc[0,0])\n",
    "estimate_std[1].iloc[0,0]=sqrt(estimate_std[1].iloc[0,0])\n",
    "for j in range(1,len(dp[0].T)):\n",
    "    for i in range(len(dp[0])):\n",
    "        estimate_std[0].iloc[i,j]=sqrt(estimate_std[0].iloc[i,j])\n",
    "for j in range(1,len(dp[1].T)):\n",
    "    for i in range(len(dp[1])):\n",
    "        estimate_std[1].iloc[i,j]=sqrt(estimate_std[1].iloc[i,j])\n",
    "    \n",
    "print \"standard error for  each record\"\n",
    "estimate_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
